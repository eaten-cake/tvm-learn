{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec57950-a9c9-4331-8cfc-8a8678191528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3919d9e7-6d79-4086-b0df-95139a8164ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da90188-8818-49b8-81cb-272994b237dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = te.var(\"n\")\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce1a761-0b1c-417f-bb0d-45e415864ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = te.create_schedule(C.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bc7eb4-0d91-4b91-9c5a-f3b295b8cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.handle, B: T.handle):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
      "        n = T.int32()\n",
      "        A_1 = T.match_buffer(A, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        B_1 = T.match_buffer(B, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        C = T.allocate([n], \"float32\", \"global\")\n",
      "        for i in range(n):\n",
      "            C_1 = T.Buffer((n,), data=C)\n",
      "            A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type=\"auto\")\n",
      "            B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type=\"auto\")\n",
      "            C_1[i] = A_2[i * A_1.strides[0]] + B_2[i * B_1.strides[0]]\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c27bfa-581a-43f2-9d94-e9bf6d03dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fadd = tvm.build(s, [A, B, C], tgt, name=\"myadd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bbe402-bfd4-484e-8158-7244e7b07dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.device(tgt.kind.name, 0)\n",
    "\n",
    "n = 1024\n",
    "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "fadd(a, b, c)\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1edcb59-f49c-4767-8029-7bdcd22d8213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.000006\n",
      "naive: 0.000004\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup=\"import numpy\\n\"\n",
    "    \"n = 32768\\n\"\n",
    "    'dtype = \"float32\"\\n'\n",
    "    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n",
    "    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n",
    "    stmt=\"answer = a + b\",\n",
    "    number=np_repeat,\n",
    ")\n",
    "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
    "\n",
    "def evaluate_addition(func, target, optimization, log):\n",
    "    dev = tvm.device(target.kind.name, 0)\n",
    "    n = 32768\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
    "    mean_time = evaluator(a, b, c).mean\n",
    "    print(\"%s: %f\" % (optimization, mean_time))\n",
    "\n",
    "    log.append((optimization, mean_time))\n",
    "\n",
    "log = [(\"numpy\", np_running_time / np_repeat)]\n",
    "evaluate_addition(fadd, tgt, \"naive\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e894503-8380-4f8b-a7a4-43d9dea2b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[C].parallel(C.op.axis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3698252d-dddc-49a1-8110-429e48b3a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.handle, B: T.handle, C: T.handle):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
      "        n = T.int32()\n",
      "        A_1 = T.match_buffer(A, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        B_1 = T.match_buffer(B, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        C_1 = T.match_buffer(C, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        for i in T.parallel(n):\n",
      "            C_2 = T.Buffer((C_1.strides[0] * n,), data=C_1.data, buffer_type=\"auto\")\n",
      "            A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type=\"auto\")\n",
      "            B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type=\"auto\")\n",
      "            C_2[i * C_1.strides[0]] = A_2[i * A_1.strides[0]] + B_2[i * B_1.strides[0]]\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208e9a52-af2c-45d6-b8c8-0a28ad6f0013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 0.000004\n"
     ]
    }
   ],
   "source": [
    "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
    "fadd_parallel(a, b, c)\n",
    "\n",
    "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "\n",
    "evaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b79e308f-392f-44a3-9813-c71d406b2807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: 0.000005\n",
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(A: T.handle, B: T.handle, C: T.handle):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
      "        n = T.int32()\n",
      "        A_1 = T.match_buffer(A, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        B_1 = T.match_buffer(B, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        C_1 = T.match_buffer(C, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
      "        for i_outer in T.parallel((n + 3) // 4):\n",
      "            for i_inner_s in range(4):\n",
      "                if T.likely(i_outer * 4 + i_inner_s < n):\n",
      "                    C_2 = T.Buffer((C_1.strides[0] * n,), data=C_1.data, buffer_type=\"auto\")\n",
      "                    A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type=\"auto\")\n",
      "                    B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type=\"auto\")\n",
      "                    cse_var_1: T.int32 = i_outer * 4 + i_inner_s\n",
      "                    C_2[cse_var_1 * C_1.strides[0]] = A_2[cse_var_1 * A_1.strides[0]] + B_2[cse_var_1 * B_1.strides[0]]\n"
     ]
    }
   ],
   "source": [
    "# 重新创建 schedule, 因为前面的例子在并行操作中修改了它\n",
    "n = te.var(\"n\")\n",
    "A = te.placeholder((n,), name=\"A\")\n",
    "B = te.placeholder((n,), name=\"B\")\n",
    "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# 这个因子应该和适合 CPU 的线程数量匹配。\n",
    "# 这会因架构差异而有所不同，不过好的规则是\n",
    "# 将这个因子设置为 CPU 可用内核数量。\n",
    "factor = 4\n",
    "\n",
    "outer, inner = s[C].split(C.op.axis[0], factor=factor)\n",
    "s[C].parallel(outer)\n",
    "s[C].vectorize(inner)\n",
    "\n",
    "fadd_vector = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
    "\n",
    "evaluate_addition(fadd_vector, tgt, \"vector\", log=log)\n",
    "\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1cd56b-494f-4c02-94ad-324e781a6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Operator\t              Timing\t         Performance\n",
      "               numpy\t5.738509998991503e-06\t                 1.0\n",
      "               naive\t3.6256999999999994e-06\t     0.6318190611565\n",
      "            parallel\t3.5552000000000003e-06\t  0.6195336421169951\n",
      "              vector\t          5.2591e-06\t  0.9164574080944783\n"
     ]
    }
   ],
   "source": [
    "baseline = log[0][1]\n",
    "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
    "for result in log:\n",
    "    print(\n",
    "        \"%s\\t%s\\t%s\"\n",
    "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef100e97-8b98-4ae0-84d9-709cfc6ec694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要运行这个代码, 更改为 `run_cuda = True`\n",
    "# 注意：默认这个示例不在 CI 文档上运行\n",
    "\n",
    "run_cuda = False\n",
    "if run_cuda:\n",
    "    # 将这个 target 改为你 GPU 的正确后端。例如：NVIDIA GPUs：cuda；Radeon GPUS：rocm；opencl：OpenCL\n",
    "    tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n",
    "\n",
    "    # 重新创建 schedule\n",
    "    n = te.var(\"n\")\n",
    "    A = te.placeholder((n,), name=\"A\")\n",
    "    B = te.placeholder((n,), name=\"B\")\n",
    "    C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
    "    print(type(C))\n",
    "\n",
    "    s = te.create_schedule(C.op)\n",
    "\n",
    "    bx, tx = s[C].split(C.op.axis[0], factor=64)\n",
    "\n",
    "    ################################################################################\n",
    "    # 最终必须将迭代轴 bx 和 tx 和 GPU 计算网格绑定。\n",
    "    # 原生 schedule 对 GPU 是无效的, 这些是允许我们生成可在 GPU 上运行的代码的特殊构造\n",
    "\n",
    "    s[C].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
    "    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
    "\n",
    "    ######################################################################\n",
    "    # 编译\n",
    "    # -----------\n",
    "    # 指定 schedule 后, 可将它编译为 TVM 函数。\n",
    "    # 默认 TVM 编译为可直接从 Python 端调用的类型擦除函数。\n",
    "    #\n",
    "    # 下面将用 tvm.build 来创建函数。\n",
    "    # build 函数接收 schedule、所需的函数签名（包括输入和输出）以及要编译到的目标语言。\n",
    "    #\n",
    "    # fadd 的编译结果是 GPU 设备函数（如果利用了 GPU）以及调用 GPU 函数的主机 wrapper。\n",
    "    # fadd 是生成的主机 wrapper 函数，它包含对内部生成的设备函数的引用。\n",
    "\n",
    "    fadd = tvm.build(s, [A, B, C], target=tgt_gpu, name=\"myadd\")\n",
    "\n",
    "    ################################################################################\n",
    "    # 编译后的 TVM 函数提供了一个任何语言都可调用的 C API。\n",
    "    #\n",
    "    # 我们在 Python 中提供了最小数组 API 来进行快速测试以及制作原型。\n",
    "    # 数组 API 基于 `DLPack [https://github.com/dmlc/dlpack](https://github.com/dmlc/dlpack)`_ 标准。\n",
    "    #\n",
    "    # - 首先创建 GPU 设备。\n",
    "    # - 然后 tvm.nd.array 将数据复制到 GPU 上。\n",
    "    # - `fadd` 运行真实的计算。\n",
    "    # - `numpy()` 将 GPU 数组复制回 CPU 上（然后验证正确性）。\n",
    "    #\n",
    "    # 注意将数据复制进出内存是必要步骤。\n",
    "\n",
    "    dev = tvm.device(tgt_gpu.kind.name, 0)\n",
    "\n",
    "    n = 1024\n",
    "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
    "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
    "    fadd(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
    "\n",
    "    ################################################################################\n",
    "    # 检查生成的 GPU 代码\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # 可以在 TVM 中检查生成的代码。tvm.build 的结果是一个 TVM 模块。fadd 是包含主机模块的主机 wrapper，对 CUDA（GPU）函数来说它还包含设备模块。\n",
    "    #\n",
    "    # 下面的代码从设备模块中取出并打印内容代码。\n",
    "\n",
    "    if (\n",
    "        tgt_gpu.kind.name == \"cuda\"\n",
    "        or tgt_gpu.kind.name == \"rocm\"\n",
    "        or tgt_gpu.kind.name.startswith(\"opencl\")\n",
    "    ):\n",
    "        dev_module = fadd.imported_modules[0]\n",
    "        print(\"-----GPU code-----\")\n",
    "        print(dev_module.get_source())\n",
    "    else:\n",
    "        print(fadd.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6b412-a4da-4d0e-ba32-4645860d3423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
