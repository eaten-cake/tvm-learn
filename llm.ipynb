{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffbf6af-5356-46d4-a406-0ea350f1cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import enum\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Optional\n",
    "\n",
    "import tvm\n",
    "from tvm import dlight, relax, te, tir\n",
    "from tvm.relax import register_pipeline\n",
    "from tvm.relax.frontend import nn\n",
    "from tvm.relax.frontend.nn import Tensor, op\n",
    "from tvm.relax.frontend.nn.llm.kv_cache import PagedKVCache, TIRPagedKVCache\n",
    "from tvm.runtime import ShapeTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1910fd85-a296-4937-bd7e-4473fd0c1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LlamaConfig:\n",
    "    hidden_size: int = 2048\n",
    "    intermediate_size: int = 5632\n",
    "    num_attention_heads: int = 32\n",
    "    num_hidden_layers: int = 22\n",
    "    rms_norm_eps: float = 1e-05\n",
    "    vocab_size: int = 32000\n",
    "    rope_theta: int = 10000\n",
    "    context_window_size: int = 2048\n",
    "    prefill_chunk_size: int = 2048\n",
    "    num_key_value_heads: int = 4\n",
    "    head_dim: int = 64  # hidden_size // num_attention_heads\n",
    "\n",
    "\n",
    "dev = tvm.device(\"cuda\", 0)\n",
    "target = tvm.target.Target.from_device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5d7078-4aa9-4678-8090-479e7f03bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopeMode(enum.IntEnum):\n",
    "    \"\"\"The RoPE mode of the Paged KV cache.\n",
    "    If it is none, the KV cache will not apply RoPE to q and k.\n",
    "    If it is normal, RoPE will be applied to k before adding k to cache.\n",
    "    Otherwise, RoPE will be applied to q/k in attention kernel on-the-fly.\n",
    "    \"\"\"\n",
    "\n",
    "    NONE = 0\n",
    "    NORMAL = 1\n",
    "    INLINE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab44604f-1fe4-4f11-8e09-4a7853cd3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaFFN(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=2 * config.intermediate_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        concat_x1_x2 = self.gate_up_proj(x)\n",
    "        x1, x2 = op.split(concat_x1_x2, 2, axis=-1)\n",
    "        return self.down_proj(op.silu(x1) * x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2cd1143-6910-45f2-951f-57121c7bf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):  # pylint: disable=too-many-instance-attributes\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_q_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        # horizontal fusion on QKV projection\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=(self.num_q_heads + 2 * self.num_kv_heads) * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.o_proj = nn.Linear(self.num_q_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, paged_kv_cache: PagedKVCache, layer_id: int):\n",
    "        d, h_q, h_kv = self.head_dim, self.num_q_heads, self.num_kv_heads\n",
    "        b, s, _ = hidden_states.shape\n",
    "        # QKV Projection\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        qkv = op.reshape(qkv, (b, s, h_q + h_kv + h_kv, d))\n",
    "        # Attention\n",
    "        output = op.reshape(\n",
    "            paged_kv_cache.attention_with_fused_qkv(layer_id, qkv, self.num_q_heads),\n",
    "            (b, s, h_q * d),\n",
    "        )\n",
    "        # Output Projection\n",
    "        return self.o_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48d8221-44fa-48f3-8fa6-2c5804893819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        rms_norm_eps = config.rms_norm_eps\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaFFN(config)\n",
    "        self.input_layernorm = nn.RMSNorm(config.hidden_size, -1, rms_norm_eps, bias=False)\n",
    "        self.post_attention_layernorm = nn.RMSNorm(config.hidden_size, -1, rms_norm_eps, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, paged_kv_cache: PagedKVCache, layer_id: int):\n",
    "        hidden_states += self.self_attn(\n",
    "            self.input_layernorm(hidden_states), paged_kv_cache, layer_id\n",
    "        )\n",
    "        hidden_states += self.mlp(self.post_attention_layernorm(hidden_states))\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = nn.RMSNorm(config.hidden_size, -1, config.rms_norm_eps, bias=False)\n",
    "\n",
    "    def forward(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n",
    "        hidden_states = input_embed\n",
    "        for layer_id, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states, paged_kv_cache, layer_id)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LlamaForCasualLM(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.dtype = \"float32\"\n",
    "\n",
    "    def to(self, dtype: Optional[str] = None):\n",
    "        super().to(dtype=dtype)\n",
    "        if dtype is not None:\n",
    "            self.dtype = dtype\n",
    "\n",
    "    def embed(self, input_ids: Tensor):\n",
    "        return self.model.embed_tokens(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_states: Tensor):\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        if logits.dtype != \"float32\":\n",
    "            logits = logits.astype(\"float32\")\n",
    "        return logits\n",
    "\n",
    "    def prefill(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n",
    "        def _index(x: te.Tensor):  # x[:-1,:]\n",
    "            b, s, d = x.shape\n",
    "            return te.compute((b, 1, d), lambda i, _, k: x[i, s - 1, k], name=\"index\")\n",
    "\n",
    "        hidden_states = self.model(input_embed, paged_kv_cache)\n",
    "        hidden_states = op.tensor_expr_op(_index, name_hint=\"index\", args=[hidden_states])\n",
    "        logits = self.get_logits(hidden_states)\n",
    "        return logits, paged_kv_cache\n",
    "\n",
    "    def decode(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n",
    "        hidden_states = self.model(input_embed, paged_kv_cache)\n",
    "        logits = self.get_logits(hidden_states)\n",
    "        return logits, paged_kv_cache\n",
    "\n",
    "    def create_tir_paged_kv_cache(\n",
    "        self,\n",
    "        max_batch_size: tir.Var,\n",
    "        max_total_seq_len: tir.Var,\n",
    "        prefill_chunk_size: tir.Var,\n",
    "        page_size: tir.Var,\n",
    "    ) -> PagedKVCache:\n",
    "        return TIRPagedKVCache(\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_total_seq_len=max_total_seq_len,\n",
    "            prefill_chunk_size=prefill_chunk_size,\n",
    "            page_size=page_size,\n",
    "            support_sliding_window=0,\n",
    "            layer_partition=relax.ShapeExpr([0, self.num_hidden_layers]),\n",
    "            num_hidden_layers=self.num_hidden_layers,\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            num_key_value_heads=self.num_key_value_heads,\n",
    "            head_dim=self.head_dim,\n",
    "            rope_mode=RopeMode.NORMAL,\n",
    "            rope_scale=1,\n",
    "            rope_theta=self.rope_theta,\n",
    "            rope_scaling={},\n",
    "            rope_ext_factors=relax.PrimValue(0),\n",
    "            rotary_dim=self.head_dim,\n",
    "            dtype=self.dtype,\n",
    "            target=target,\n",
    "        )\n",
    "\n",
    "    def get_default_spec(self):\n",
    "        mod_spec = {\n",
    "            \"embed\": {\n",
    "                \"input_ids\": nn.spec.Tensor([\"seq_len\"], \"int32\"),\n",
    "                \"$\": {\n",
    "                    \"param_mode\": \"packed\",\n",
    "                    \"effect_mode\": \"none\",\n",
    "                },\n",
    "            },\n",
    "            \"prefill\": {\n",
    "                \"input_embed\": nn.spec.Tensor([1, \"seq_len\", self.hidden_size], self.dtype),\n",
    "                \"paged_kv_cache\": nn.spec.Object(object_type=PagedKVCache),\n",
    "                \"$\": {\n",
    "                    \"param_mode\": \"packed\",\n",
    "                    \"effect_mode\": \"none\",\n",
    "                },\n",
    "            },\n",
    "            \"decode\": {\n",
    "                \"input_embed\": nn.spec.Tensor([1, 1, self.hidden_size], self.dtype),\n",
    "                \"paged_kv_cache\": nn.spec.Object(object_type=PagedKVCache),\n",
    "                \"$\": {\n",
    "                    \"param_mode\": \"packed\",\n",
    "                    \"effect_mode\": \"none\",\n",
    "                },\n",
    "            },\n",
    "            \"create_tir_paged_kv_cache\": {\n",
    "                \"max_batch_size\": int,\n",
    "                \"max_total_seq_len\": int,\n",
    "                \"prefill_chunk_size\": int,\n",
    "                \"page_size\": int,\n",
    "                \"$\": {\n",
    "                    \"param_mode\": \"none\",\n",
    "                    \"effect_mode\": \"none\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        return nn.spec.ModuleSpec.from_raw(mod_spec, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44419e43-bdf4-40f6-ab1e-3ac3450410ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@R.function\n",
      "def prefill(input_embed: R.Tensor((1, \"seq_len\", 2048), dtype=\"float16\"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor((32000, 2048), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2560, 2048), dtype=\"float16\"), R.Tensor((2048, 2048), dtype=\"float16\"), R.Tensor((11264, 2048), dtype=\"float16\"), R.Tensor((2048, 5632), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((2048,), dtype=\"float16\"), R.Tensor((32000, 2048), dtype=\"float16\"))) -> R.Tuple(R.Tensor((1, 1, 32000), dtype=\"float32\"), R.Object):\n",
      "    seq_len = T.int64()\n",
      "    R.func_attr({\"num_input\": 2})\n",
      "    with R.dataflow():\n",
      "        model_embed_tokens_weight1: R.Tensor((32000, 2048), dtype=\"float16\") = packed_params[0]\n",
      "        model_layers_0_self_attn_qkv_proj_weight1: R.Tensor((2560, 2048), dtype=\"float16\") = packed_params[1]\n",
      "        model_layers_0_self_attn_o_proj_weight1: R.Tensor((2048, 2048), dtype=\"float16\") = packed_params[2]\n",
      "        model_layers_0_mlp_gate_up_proj_weight1: R.Tensor((11264, 2048), dtype=\"float16\") = packed_params[3]\n",
      "        model_layers_0_mlp_down_proj_weight1: R.Tensor((2048, 5632), dtype=\"float16\") = packed_params[4]\n",
      "        model_layers_0_input_layernorm_weight1: R.Tensor((2048,), dtype=\"float16\") = packed_params[5]\n",
      "        model_layers_0_post_attention_layernorm_weight1: R.Tensor((2048,), dtype=\"float16\") = packed_params[6]\n",
      "        model_layers_1_self_attn_qkv_proj_weight1: R.Tensor((2560, 2048), dtype=\"float16\") = packed_params[7]\n",
      "        model_layers_1_self_attn_o_proj_weight1: R.Tensor((2048, 2048), dtype=\"float16\") = packed_params[8]\n",
      "        model_layers_1_mlp_gate_up_proj_weight1: R.Tensor((11264, 2048), dtype=\"float16\") = packed_params[9]\n",
      "        model_layers_1_mlp_down_proj_weight1: R.Tensor((2048, 5632), dtype=\"float16\") = packed_params[10]\n",
      "        model_layers_1_input_layernorm_weight1: R.Tensor((2048,), dtype=\"float16\") = packed_params[11]\n",
      "        ...\n",
      "\n",
      "Parameters:\n",
      "[('model.embed_tokens.weight', Tensor([32000, 2048], \"float16\")),\n",
      " ('model.layers.0.self_attn.qkv_proj.weight', Tensor([2560, 2048], \"float16\")),\n",
      " ('model.layers.0.self_attn.o_proj.weight', Tensor([2048, 2048], \"float16\")),\n",
      " ('model.layers.0.mlp.gate_up_proj.weight', Tensor([11264, 2048], \"float16\")),\n",
      " ('model.layers.0.mlp.down_proj.weight', Tensor([2048, 5632], \"float16\"))]\n"
     ]
    }
   ],
   "source": [
    "model_config = LlamaConfig()\n",
    "model = LlamaForCasualLM(model_config)\n",
    "model.to(\"float16\")\n",
    "mod, named_params = model.export_tvm(spec=model.get_default_spec())\n",
    "prefill_str = mod[\"prefill\"].script()\n",
    "print(*prefill_str.split(\"\\n\")[3:20], sep=\"\\n\")  # Only show the first 10 lines for demonstration\n",
    "print(\"        ...\")\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "pprint(named_params[:5])  # Only show the first 5 parameters for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce21b2ee-1670-492e-87f7-1d1de14ad31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pipeline(\"opt_llm\")\n",
    "def _pipeline(  # pylint: disable=too-many-arguments\n",
    "    ext_mods: List[nn.ExternModule] = None,\n",
    "):\n",
    "    ext_mods = ext_mods or []\n",
    "\n",
    "    @tvm.transform.module_pass(opt_level=0)\n",
    "    def _pipeline(mod: tvm.ir.IRModule, _ctx: tvm.transform.PassContext) -> tvm.ir.IRModule:\n",
    "        seq = tvm.transform.Sequential(\n",
    "            [\n",
    "                # Phase 1. Passes on high-level operator graph\n",
    "                # We can enable cublas for further optimization\n",
    "                relax.transform.FuseTransposeMatmul(),\n",
    "                # Phase 2. Lowering to TIR, inherited TVM Relax's official \"zero\" pipeline\n",
    "                relax.transform.LegalizeOps(),\n",
    "                relax.transform.AnnotateTIROpPattern(),\n",
    "                relax.transform.FoldConstant(),\n",
    "                relax.transform.FuseOps(),\n",
    "                relax.transform.FuseTIR(),\n",
    "                # Phase 3. Passes on TIR\n",
    "                relax.transform.DeadCodeElimination(),\n",
    "                # Phase 4. Low-level Optimizations\n",
    "                dlight.ApplyDefaultSchedule(\n",
    "                    dlight.gpu.Matmul(),\n",
    "                    dlight.gpu.GEMV(),\n",
    "                    dlight.gpu.Reduction(),\n",
    "                    dlight.gpu.GeneralReduction(),\n",
    "                    dlight.gpu.Fallback(),\n",
    "                ),\n",
    "                # Phase 5. Lowering to VM bytecode\n",
    "                relax.transform.RewriteDataflowReshape(),\n",
    "                relax.transform.ToNonDataflow(),\n",
    "                relax.transform.RemovePurityChecking(),\n",
    "                relax.transform.CallTIRRewrite(),\n",
    "                relax.transform.StaticPlanBlockMemory(),\n",
    "                relax.transform.RewriteCUDAGraph(),\n",
    "                relax.transform.LowerAllocTensor(),\n",
    "                relax.transform.KillAfterLastUse(),\n",
    "                relax.transform.LowerRuntimeBuiltin(),\n",
    "                relax.transform.VMShapeLower(),\n",
    "                relax.transform.AttachGlobalSymbol(),\n",
    "                relax.transform.AttachExternModules(ext_mods),\n",
    "            ]\n",
    "        )\n",
    "        mod = seq(mod)\n",
    "        return mod\n",
    "\n",
    "    return _pipeline\n",
    "\n",
    "\n",
    "with target:\n",
    "    ex = relax.build(mod, target, pipeline=relax.get_pipeline(\"opt_llm\"))\n",
    "    vm = relax.VirtualMachine(ex, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d42769-fba8-4eb8-a41d-df669ae147cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n",
    "\n",
    "HF_WEIGHT_PATH = None\n",
    "HF_WEIGHT_PATH = Path(\"/home/yrx/model/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "if not IS_IN_CI:\n",
    "    import numpy as np\n",
    "    import safetensors.torch\n",
    "    import torch\n",
    "\n",
    "    if HF_WEIGHT_PATH is None or not HF_WEIGHT_PATH.exists():\n",
    "        raise ValueError(\"Please set the HF_WEIGHT_PATH to the path of the pre-trained weights.\")\n",
    "\n",
    "    # Torch format weights\n",
    "    param_dict = safetensors.torch.load_file(HF_WEIGHT_PATH / \"model.safetensors\", device=\"cpu\")\n",
    "    # Numpy format weights\n",
    "    param_dict = {\n",
    "        k: v.half().numpy() if v.dtype == torch.bfloat16 else v.numpy()\n",
    "        for k, v in param_dict.items()\n",
    "    }\n",
    "\n",
    "    named_params = dict(named_params)\n",
    "    for i in range(model_config.num_hidden_layers):\n",
    "        # Add QKV in self attention\n",
    "        attn = f\"model.layers.{i}.self_attn\"\n",
    "        param_dict[f\"{attn}.qkv_proj.weight\"] = np.concatenate(\n",
    "            [\n",
    "                param_dict.pop(f\"{attn}.q_proj.weight\"),  # Pop the old parameters to save memory\n",
    "                param_dict.pop(f\"{attn}.k_proj.weight\"),\n",
    "                param_dict.pop(f\"{attn}.v_proj.weight\"),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        # Add gates in MLP\n",
    "        mlp = f\"model.layers.{i}.mlp\"\n",
    "        param_dict[f\"{mlp}.gate_up_proj.weight\"] = np.concatenate(\n",
    "            [\n",
    "                param_dict.pop(f\"{mlp}.gate_proj.weight\"),\n",
    "                param_dict.pop(f\"{mlp}.up_proj.weight\"),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    # Convert params into ndarray\n",
    "    params = [\n",
    "        tvm.nd.array(param_dict[k].astype(\"float16\"), device=dev) for k in named_params.keys()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "030bd916-ee13-4785-9c84-24879587414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_IN_CI:\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_WEIGHT_PATH)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, How are you?\"},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages)\n",
    "    input_len = len(prompt)\n",
    "\n",
    "    # Load prompt tokens into TVM ndarray on the target device\n",
    "    tokens = tvm.nd.array(np.array(prompt).astype(\"int32\"), device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0781d2d-8aea-4453-a221-d489e2ed6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_IN_CI:\n",
    "    kv_cache = vm[\"create_tir_paged_kv_cache\"](\n",
    "        ShapeTuple([1]),  # max_batch_size=1\n",
    "        ShapeTuple([2048]),  # max_total_seq_len=2048\n",
    "        ShapeTuple([2048]),  # prefill_chunk_size=2048\n",
    "        ShapeTuple([16]),  # page_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef4193bf-7ee1-43e7-9817-bf6c5ed205cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_view_func = tvm.get_global_func(\"vm.builtin.reshape\")\n",
    "\n",
    "\n",
    "def embed(tokens, params):\n",
    "    _embed = vm[\"embed\"](tokens, params)\n",
    "    # Reshape hidden from [seq_len, hidden_size] to [1, seq_len, hidden_size]\n",
    "    _embed = nd_view_func(_embed, ShapeTuple([1, _embed.shape[0], _embed.shape[1]]))\n",
    "    return _embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af126ae4-93ca-4b35-9438-5051ae950845",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sequence_func = tvm.get_global_func(\"vm.builtin.kv_state_add_sequence\")\n",
    "begin_forward_func = tvm.get_global_func(\"vm.builtin.kv_state_begin_forward\")\n",
    "end_forward_func = tvm.get_global_func(\"vm.builtin.kv_state_end_forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c055721f-aa21-4e3d-8aa6-8c76e8c9136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_IN_CI:\n",
    "    seq_id = 0\n",
    "    add_sequence_func(kv_cache, seq_id)\n",
    "    hidden_states = embed(tokens, params)\n",
    "    begin_forward_func(kv_cache, ShapeTuple([seq_id]), ShapeTuple([input_len]))\n",
    "    logits, kv_cache = vm[\"prefill\"](hidden_states, kv_cache, params)\n",
    "    end_forward_func(kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3859f10c-4a2f-4ff5-91e6-cdeb2992f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "    logits_np = logits.numpy()\n",
    "    return np.argmax(logits_np)\n",
    "\n",
    "\n",
    "if not IS_IN_CI:\n",
    "    last_token = sample_token(logits)\n",
    "    output_tokens = [last_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "386ca350-6de5-4329-95dd-985d7c9ad439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated token:\n",
      "<|assistant|>\n",
      "I am fine, thank you. How are you?\n",
      "\n",
      "responding to a question with a statement is known as a direct question. Direct questions are usually asked to get information or to clarify a statement. In this case, the question \"How are you?\" is a direct question that asks for someone's current or current state of being. It is a simple and direct way to ask someone how they are feeling or what they are doing.</s>\n"
     ]
    }
   ],
   "source": [
    "if not IS_IN_CI:\n",
    "    print(\"The generated token:\")\n",
    "\n",
    "    while last_token != tokenizer.eos_token_id:\n",
    "        tokens = tvm.nd.array(np.array([last_token]).astype(\"int32\"), device=dev)\n",
    "        hidden_states = embed(tokens, params)\n",
    "        begin_forward_func(kv_cache, ShapeTuple([seq_id]), ShapeTuple([1]))\n",
    "        logits, kv_cache = vm[\"decode\"](hidden_states, kv_cache, params)\n",
    "\n",
    "        end_forward_func(kv_cache)\n",
    "        last_token = sample_token(logits)\n",
    "        output_tokens.append(last_token)\n",
    "\n",
    "    print(tokenizer.decode(output_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
